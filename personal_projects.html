<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Personal Projects - Josh Aney</title>
    <link rel="stylesheet" href="css/style.css">
</head>
<body>

<script src="js/slideshow.js"></script>

<header>
    <nav>
        <ul>
            <li><a href="index.html">About Me</a></li>
            <li><a href="personal_projects.html">Personal Projects</a></li>
            <li><a href="images/resume.pdf" target="_blank">Resume</a></li>
        </ul>
    </nav>
</header>

<main>

    <h1>Personal Projects</h1>
    <p>This page is meant to give a short descriptions of many of the projects that I have been a part of. </p>

    <p>There are four machine learning projects that are listed. These projects are set up in an experiment style where
    a question was prompted and then an experiment was completed to test our hypotheses. A full report will be linked
    after each project. This report will contain a detailed explanation of the problem, the methods that were used in the
    experiment, and a detailed analysis of the results</p>

    <p>The next project is a small web application that is meant to help a user track what trails they have ridden in
    the Bozeman area. This application lets users add trails which the trails will then be plotted on a 3d map as well
    as being displayed on the home page. </p>

    <p>The final project is a small unity game that allows the user to rol a ball around a surface while trying to
        collect  pick-ups. There is a antagonist that will chase the user while they are trying to pick up the
        collectibles.</p>

    <p>All the source code for these projects can be found on my <a href="https://github.com/joshaney324">GitHub</a>.</p>

    <div class="project-list">
        <ul>
            <li>
                <strong>Project 1:</strong> <a href="#ml_1">Naive Bayes Model From Scratch</a>
                <ul>
                    <li>Python, UML, Numpy</li>
                    <li>Probability Theory</li>
                    <li>Data Preprocessing: Data Imputation, Binning</li>
                    <li>Technical Writing</li>
                </ul>
            </li>
            <li>
                <strong>Project 2:</strong> <a href="#ml_2">K-Nearest-Neighbors Methods From Scratch</a>
                <ul>
                    <li>Python, UML, Numpy</li>
                    <li>Data Preprocessing: Data Imputation, One-Hot Encoding, Data Normalization</li>
                    <li>K-Nearest-Neighbors Classification and Regression</li>
                    <li>Edited K-Nearest-Neighbors Classification and Regression</li>
                    <li>K-Means Clustering</li>
                    <li>Technical Writing</li>
                </ul>
            </li>
            <li>
                <strong>Project 3:</strong> <a href="#ml_3">Neural Network From Scratch</a>
                <ul>
                    <li>Python, UML, NumPy</li>
                    <li>Data Preprocessing: Data Imputation, One-Hot Encoding, Data Normalization</li>
                    <li>Backpropagation</li>
                    <li>Technical Writing</li>
                </ul>
            </li>
            <li>
                <strong>Project 4:</strong> <a href="#ml_4">Experimenting With Neural Network Training Methods</a>
                <ul>
                    <li>Python, UML, NumPy</li>
                    <li>Data Preprocessing: Data Imputation, One-Hot Encoding, Data Normalization</li>
                    <li>Backpropagation</li>
                    <li>Genetic Algorithm</li>
                    <li>Particle Swarm Optimization</li>
                    <li>Technical Writing</li>
                </ul>
            </li>
            <li>
                <strong>Project 5:</strong> <a href="#bike-webapp">Bozeman Bike Trails Web Application</a>
                <ul>
                    <li>C#, HTML, CSS, SQL</li>
                    <li>ASP.NET Core</li>
                    <li>Entity Framework Core</li>
                    <li>Mapbox, GPX Parsing</li>
                </ul>
            </li>
            <li>
                <strong>Project 6:</strong> <a href="#unity">Unity Project</a>
                <ul>
                    <li>C#</li>
                    <li>Unity Physics</li>
                    <li>Animation and UI Design</li>
                </ul>
            </li>
        </ul>
    </div>


    <div id="ml_1" class="container">
        <section class="project">
            <h2>Machine Learning Project 1 - Naive Bayes</h2>
            <p><strong>Description:</strong> For this project, I worked in a team with another student at Montana State University. We
                were faced with coding a Naive Bayes model from scratch to be used on classification datasets from the
                <a href="https://archive.ics.uci.edu/">UCI Machine Learning Repository</a>. For this project, we used the
                <a href="https://archive.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original">Breast Cancer Wisconsin Dataset</a>,
                <a href="https://archive.ics.uci.edu/dataset/42/glass+identification">Glass Identification Dataset</a>,
                <a href="https://archive.ics.uci.edu/dataset/105/congressional+voting+records">Congressional Voting Records Dataset</a>,
                <a href="https://archive.ics.uci.edu/dataset/53/iris">Iris Dataset</a>, and
                <a href="https://archive.ics.uci.edu/dataset/91/soybean+small">Soybean (Small) Dataset</a>. A base model
                performance was received after testing each of the datasets. We performed 10-fold cross validation, and
                we used precision recall and accuracy as the metrics. Once we had a base performance, we then added
                artificial noise to the datasets. This was done by selecting one feature in the dataset at random and
                then mixing up the order of that given attribute. We then performed the experiment one more time to test
                how our Naive Bayes model performed with more noise.</p>
            <br>
            <p>The full design document for this project can be found <a href="images/ml_1/Project%201%20Design%20Document.pdf" target="_blank">here</a>.</p>
            <br>
            <p><strong>Results:</strong> Our Naive Bayes model performed well on all the datasets with a slight decrease in performance with the simulated noise.</p>


            <div class="slideshow-container ml_1">

                <div class="mySlides fade" data-caption="Original Data Averages">
                    <img src="images/ml_1/original_data_avgs.svg" class="slideshow-img">
                </div>

                <div class="mySlides fade" data-caption="Noisy Data Averages">
                    <img src="images/ml_1/noisy_data_avgs.svg" class="slideshow-img">
                </div>

                <div style="padding-top: 20px">
                    <a class="prev" onclick="showSlide('.ml_1', -1)">&#10094;</a>
                    <a class="next" onclick="showSlide('.ml_1', 1)">&#10095;</a>
                    <span class="caption"></span>
                </div>

            </div>
            <br>

            <p>The full results from this experiment can be found <a href="images/ml_1/ml_paper_1.pdf" target="_blank">here</a></p>
        </section>
    </div>

    <div id="ml_2" class="container">
        <section class="project">
            <h2>Machine Learning Project 2 - K Nearest Neighbors Methods</h2>
            <p><strong>Description</strong>: For this project, I worked in a team with two other students at Montana State University.
                We were tasked with coding a KNN classification model, KNN regression model, edited KNN classification
                model, edited KNN regression model, and a K-Means clustering model from scratch. We then used these
                models on a variety of datasets. We used the
                <a href="https://archive.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original">Breast Cancer Wisconsin Dataset</a>,
                <a href="https://archive.ics.uci.edu/dataset/42/glass+identification">Glass Identification Dataset</a>,
                <a href="https://archive.ics.uci.edu/dataset/91/soybean+small">Soybean (Small) Dataset</a>,
                <a href="https://archive.ics.uci.edu/dataset/1/abalone">Abalone Dataset</a>,
                <a href="https://archive.ics.uci.edu/dataset/162/forest+fires">Forest Fires Dataset</a>, and the
                <a href="https://archive.ics.uci.edu/dataset/29/computer+hardware">Computer Hardware Dataset</a>.
                For the classification datasets, a base KNN classification model was created. To do this we took out a
                hold-out fold from the data to tune all the hyperparameters for the model. Once the model was tuned, 10-fold
                cross validation was completed with the remaining data. There was no removal of noisy data during this run.
                We then completed another run of this experiment, but we used an edited KNN classification model, so the
                model would remove data points that it got wrong. This in turn ended up removing noise. A similar process
                was completed for the regression datasets, but the only difference in the model was that a kernel was
                used to weight the distance of the K closest neighbors instead of just going with the mean of the k
                closest neighbors. For the edited KNN regression model, we had to tune a threshold for how close the
                prediction was to the actual value to decide if the model would remove the datapoint. Finally, we
                performed the experiment one more time, but instead of using edited KNN models to find the edited dataset,
                we used the centroids from a K-Means Clustering model. Once these experiments were completed, we then
                analyzed and compared the results from all the different models</p>
            <br>
            <p>The full design document for this project can be found <a href="images/ml_2/Project%202%20Design%20Document.pdf" target="_blank">here</a>.</p>
            <br>
            <p><strong>Results</strong>: Many of the K Nearest Neighbors Methods performed better on classification data then on regression data.
                The performance of individual models depended on the structure of each dataset. Depending on if the dataset
                contained large amounts of outliers, the K-Means model or the edited model would perform better.</p>


            <div class="slideshow-container ml_2">

                <div class="mySlides fade" data-caption="Breast Cancer Dataset Performance">
                    <img src="images/ml_2/BreastCancerBarPlot.png" class="slideshow-img">
                </div>

                <div class="mySlides fade" data-caption="Soybean (small) Dataset Performance">
                    <img src="images/ml_2/SoyBarPlot.png" class="slideshow-img">
                </div>

                <div class="mySlides fade" data-caption="Glass Dataset Performance">
                    <img src="images/ml_2/GlassBarPlot.png" class="slideshow-img">
                </div>

                <div class="mySlides fade" data-caption="Abalone Dataset Performance">
                    <img src="images/ml_2/AbaloneBarPlot.png" class="slideshow-img">
                </div>

                <div class="mySlides fade" data-caption="Computer Hardware Dataset Performance">
                    <img src="images/ml_2/HardwareBarPlot.png" class="slideshow-img">
                </div>

                <div class="mySlides fade" data-caption="Forest Fire Dataset Performance">
                    <img src="images/ml_2/ForestBarPlot.png" class="slideshow-img">
                </div>

                <div style="padding-top: 20px">
                    <a class="prev" onclick="showSlide('.ml_2', -1)">&#10094;</a>
                    <a class="next" onclick="showSlide('.ml_2', 1)">&#10095;</a>
                    <span class="caption"></span>
                </div>

            </div>
            <br>

            <p>The full results from this experiment can be found <a href="images/ml_2/ml_paper_2.pdf" target="_blank">here</a></p>
        </section>
    </div>

    <div id="ml_3" class="container">
        <section class="project">
            <h2>Machine Learning Project 3 - Densely Connected Neural Network</h2>
            <p><strong>Description</strong>: For this project, I worked in a team with two other students at Montana State University.
                We were tasked with coding a densely connected neural network. We used the
                <a href="https://archive.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original">Breast Cancer Wisconsin Dataset</a>,
                <a href="https://archive.ics.uci.edu/dataset/42/glass+identification">Glass Identification Dataset</a>,
                <a href="https://archive.ics.uci.edu/dataset/91/soybean+small">Soybean (Small) Dataset</a>,
                <a href="https://archive.ics.uci.edu/dataset/1/abalone">Abalone Dataset</a>,
                <a href="https://archive.ics.uci.edu/dataset/162/forest+fires">Forest Fires Dataset</a>, and the
                <a href="https://archive.ics.uci.edu/dataset/29/computer+hardware">Computer Hardware Dataset</a>.
                The network that was created to be used on this data was a densely connected network with no bias nodes.
                Standard backpropagation was then used to train the network based off of the loss functions that we
                designated each dataset. Once this was done, we tuned learning rate, and the number of nodes in each
                hidden layer for zero, one and two hidden layers based off of each dataset. The tuning was done in the
                same manner as it was done in the KNN methods project with a hold-out fold. An experiment was then conducted to analyze the performance of our neural network with varying network
                sizes and varying amounts of hidden layers on each dataset. Each dataset was tested by completing 10-fold
                cross validation excluding the hold out fold's data points. These results were then analyzed for each dataset.</p>
            <br>
            <p>The full design document for this project can be found <a href="images/ml_3/Project%203%20Design%20Document.pdf" target="_blank">here</a>.</p>
            <br>
            <p><strong>Results</strong>: The neural network tended to perform better with zero or one hidden layer,
                compared to two hidden layers. On the more complex datasets, the network may perform better with one
                hidden layer, but it rarely performed best with two hidden layers.</p>

            <div class="slideshow-container ml_3">

                <div class="mySlides fade" data-caption="Breast Cancer Dataset Performance">
                    <img src="images/ml_3/breast_bar.svg" class="slideshow-img">
                </div>

                <div class="mySlides fade" data-caption="Soybean (small) Dataset Performance">
                    <img src="images/ml_3/soy_bar.svg" class="slideshow-img">
                </div>

                <div class="mySlides fade" data-caption="Glass Dataset Performance">
                    <img src="images/ml_3/glass_bar.svg" class="slideshow-img">
                </div>

                <div class="mySlides fade" data-caption="Abalone Dataset Performance">
                    <img src="images/ml_3/abalone_bar.svg" class="slideshow-img">
                </div>

                <div class="mySlides fade" data-caption="Computer Hardware Dataset Performance">
                    <img src="images/ml_3/machine_bar.svg" class="slideshow-img">
                </div>

                <div class="mySlides fade" data-caption="Forest Fire Dataset Performance">
                    <img src="images/ml_3/forest_bar.svg" class="slideshow-img">
                </div>

                <div style="padding-top: 20px">
                    <a class="prev" onclick="showSlide('.ml_3', -1)">&#10094;</a>
                    <a class="next" onclick="showSlide('.ml_3', 1)">&#10095;</a>
                    <span class="caption"></span>
                </div>

            </div>

            <p>The full results from this experiment can be found <a href="images/ml_3/ml_paper_3.pdf" target="_blank">here</a></p>
        </section>
    </div>

    <div id="ml_4" class="container">
        <section class="project">
            <h2>Machine Learning Project 4 - A Comparison of Neural Network Training Methods</h2>
            <p><strong>Description</strong>: For this project, I worked in a team with two other students at Montana State University.
                We were tasked with coding multiple different training methods for a neural network. We used the
                <a href="https://archive.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original">Breast Cancer Wisconsin Dataset</a>,
                <a href="https://archive.ics.uci.edu/dataset/42/glass+identification">Glass Identification Dataset</a>,
                <a href="https://archive.ics.uci.edu/dataset/91/soybean+small">Soybean (Small) Dataset</a>,
                <a href="https://archive.ics.uci.edu/dataset/1/abalone">Abalone Dataset</a>,
                <a href="https://archive.ics.uci.edu/dataset/162/forest+fires">Forest Fires Dataset</a>, and the
                <a href="https://archive.ics.uci.edu/dataset/29/computer+hardware">Computer Hardware Dataset</a>.
                This project then built on project 3. We used the same network structure that was created in project 3,
                but this time we intended on testing other training methods. We used backpropagation as our baseline to
                compare to, then we trained the network by the genetic algorithm, differential evolution, and particle
                swarm optimization. Because of the complexity of these algorithms and the computational power needed,
                we only tuned a small amount of the available hyperparameters. Once again we performed 10-fold cross
                validation on all the datasets and then compared the results across all the training methods.</p>
            <br>
            <p>The full design document for this project can be found <a href="images/ml_4/ML%20Project%204%20Design%20Doc.pdf" target="_blank">here</a>.</p>
            <br>
            <p><strong>Results</strong>: The performance of the models was really variable. Datasets that tended to
                overfit with backpropagation would often perform better with population based methods and datasets that
                were relatively simple would often perform drastically better with backpropagation.</p>
            <div class="slideshow-container ml_4">

                <div class="mySlides fade" data-caption="Breast Cancer Dataset Performance">
                    <img src="images/ml_4/BreastCancerAccuracy.png" class="slideshow-img">
                </div>

                <div class="mySlides fade" data-caption="Soybean (small) Dataset Performance">
                    <img src="images/ml_4/SoyAccuracy.png" class="slideshow-img">
                </div>

                <div class="mySlides fade" data-caption="Glass Dataset Performance">
                    <img src="images/ml_4/GlassAccuracy.png" class="slideshow-img">
                </div>

                <div class="mySlides fade" data-caption="Abalone Dataset Performance">
                    <img src="images/ml_4/AbaloneMSE.png" class="slideshow-img">
                </div>

                <div class="mySlides fade" data-caption="Computer Hardware Dataset Performance">
                    <img src="images/ml_4/MachineMSE.png" class="slideshow-img">
                </div>

                <div class="mySlides fade" data-caption="Forest Fire Dataset Performance">
                    <img src="images/ml_4/ForestMSE.png" class="slideshow-img">
                </div>

                <div style="padding-top: 20px">
                    <a class="prev" onclick="showSlide('.ml_4', -1)">&#10094;</a>
                    <a class="next" onclick="showSlide('.ml_4', 1)">&#10095;</a>
                    <span class="caption"></span>
                </div>

            </div>

            <p>The full results from this experiment can be found <a href="images/ml_4/ml_paper_4.pdf" target="_blank">here</a></p>
        </section>
    </div>

    <div id="bike-webapp" class="container">
        <section class="project">
            <h2>Web Project</h2>
            <p><strong>Description</strong>: I created a basic web application that allows users to log different mountain bike trails in
                the Bozeman community. A user is able to keep a list of trails that they have ridden as well as plot it
                on a 3d map. Once they add the trails that they have ridden, it will show up on the home page with the
                description, difficulty, as well as other trail descriptors. Users are also able to search for trails
                based off of trail name, trail difficulty, and trail type. </p>

            <div class="slideshow-container bike_webapp">

                <div class="mySlides fade" data-caption="Website Homepage">
                    <img src="images/bike_webapp/home_page.png" class="slideshow-img">
                </div>

                <div class="mySlides fade" data-caption="Login Page">
                    <img src="images/bike_webapp/login_page.png" class="slideshow-img">
                </div>

                <div class="mySlides fade" data-caption="Adding Trails">
                    <img src="images/bike_webapp/add_trail.png" class="slideshow-img">
                </div>

                <div class="mySlides fade" data-caption="Trail Search">
                    <img src="images/bike_webapp/trail_search.png" class="slideshow-img">
                </div>

                <div class="mySlides fade" data-caption="Trail Map">
                    <img src="images/bike_webapp/trail_map.png" class="slideshow-img">
                </div>

                <div style="padding-top: 20px">
                    <a class="prev" onclick="showSlide('.bike_webapp', -1)">&#10094;</a>
                    <a class="next" onclick="showSlide('.bike_webapp', 1)">&#10095;</a>
                    <span class="caption"></span>
                </div>
            </div>

            <p><strong>Technologies</strong>: This website is built off ASP .NET Core. It uses razor pages to build the
                structure of the website.</p>
            <p><strong>Future Additions</strong>: Eventually I would like to functionality for multiple users so that
                individual users would be able to have their own trails logged as well as not having a public database.
                I would also like to add more features to the map so that it can highlight trails based off of the user's
                searches.</p>
        </section>
    </div>

    <div id="unity" class="container">
        <section class="project">
            <h2>Unity Project</h2>
            <p><strong>Description</strong>: This project is currently a work in progress. I am working on building a
                game that can be learned by an AI agent. Currently, the base game is built with user input in mind
                instead of an AI agent. The goal of the game is to pick up collectibles and not get caught by an
                antagonist.</p>

            <div class="slideshow-container unity">

                <div class="mySlides fade" data-caption="Unity UI">
                    <img src="images/roll_a_ball/ui.png" class="slideshow-img">
                </div>

                <div class="mySlides fade" data-caption="Roll A Ball gameplay">
                    <img src="images/roll_a_ball/gameplay.png" class="slideshow-img">
                </div>

                <div style="padding-top: 20px">
                    <a class="prev" onclick="showSlide('.unity', -1)">&#10094;</a>
                    <a class="next" onclick="showSlide('.unity', 1)">&#10095;</a>
                    <span class="caption"></span>
                </div>

            </div>

            <p><strong>Technologies</strong>: This game is built with Unity 6 as well as C# for all the game controllers. The future AI
                Agent will be built with Unity ML Agents.</p>
            <p><strong>Future Additions</strong>: As stated above the goal will be to build a PPO (Proximal Policy Optimization)
                model to train an AI Agent to beat the game. To do this I will need to build randomized maps as well as
                randomized collectible placements to allow the model to not overfit.</p>
        </section>
    </div>

</main>

<footer class="footer">
    <div class="footer-bottom">
        <p>© 2024 Josh Aney | <a href="mailto:josh.aney@icloud.com">josh.aney@icloud.com</a> | <a href="tel:+5075130517">(507)-513-0517</a> | GitHub: <a href="https://github.com/joshaney324">joshaney324</a></p>
    </div>
</footer>


</body>
</html>
